{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1da3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46167c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fa267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  3 13:21:23 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 532.03                 Driver Version: 532.03       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080       WDDM | 00000000:0A:00.0  On |                  N/A |\n",
      "|  0%   46C    P8               42W / 390W|    736MiB / 12288MiB |     12%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4056    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      4860    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      6448    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11704    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13996    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18204    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     19248    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     19252    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     19504    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21332    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     22668    C+G   ...B\\system_tray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     22764    C+G   C:\\Program Files\\LGHUB\\lghub.exe          N/A      |\n",
      "|    0   N/A  N/A     23912    C+G   ...al\\Discord\\app-1.0.9013\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     24928    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     25352    C+G   ...amsung Magician\\SamsungMagician.exe    N/A      |\n",
      "|    0   N/A  N/A     26020    C+G   ...03.0_x64__8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     26432    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     26948    C+G   ....0_x64__kzh8wxbdkxb8p\\DCv2\\DCv2.exe    N/A      |\n",
      "|    0   N/A  N/A     27144    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     27236    C+G   ..._x64__kzf8qxf38zg5c\\Skype\\Skype.exe    N/A      |\n",
      "|    0   N/A  N/A     27552    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4f9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracket notation helps to define the dimensions of a tensor in PyTorch\n",
    "x = [[1, 2],[3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b805b7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0555, 0.5620, 0.1665],\n",
      "        [0.6166, 0.3720, 0.0744],\n",
      "        [0.8972, 0.2998, 0.0614],\n",
      "        [0.6991, 0.8086, 0.7966],\n",
      "        [0.2086, 0.2880, 0.1253]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3) # rand{0,1} 5 rows, 3 columns\n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54741a",
   "metadata": {},
   "source": [
    "### Why random tensors?\n",
    "Random tensors are important because many neural networks are initialized with random numbers, i.e. parameters weights and biases, and adjust these random numbers to better represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332e366e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1280, 720]), 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of bracket notation and concept of a 3dim-tensor: 2D RGB image\n",
    "random_image_tensor = torch.rand(size=(3, 1280, 720)) # RGB color channel, height (pixel), width (pixel)\n",
    "random_image_tensor.shape, random_image_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e049b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random image\n",
    "### Add code later ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098c5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Syntax notes ##\n",
    "# torch.rand(X, Y, Z) == torch.rand(size=X, Y, Z), tensor of X * Y * Z dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fe3506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all zeros. \n",
    "zeros = torch.zeros(size=(3, 1280, 720))\n",
    "zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144dea8a",
   "metadata": {},
   "source": [
    "### Notes - Zero tensors\n",
    "Zero tensors can function as a mask i.e. setting a column, row, etc, or the entire tensor of a target tensor to 0, using matrix mult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cecaad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using zero tensor as a mask\n",
    "zeros*random_image_tensor # ... the same result as the zeros tensor above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d013313b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all ones\n",
    "ones = torch.ones(size=(3, 1280, 720))\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b652059",
   "metadata": {},
   "source": [
    "### Notes - Random, Zeros and Ones tensors\n",
    "In practice, random tensors are used a lot, so are zeros tensor. Less commonly one tensors are used. But it stands to reason that Ones and Zeros tensors together could be use in conjunction to create manually masks for sparse solutions to a deep learning neural network, or to specify a neural network connectivity (graph) matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f22709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "tensor([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100])\n"
     ]
    }
   ],
   "source": [
    "# Use torch.arange(start, end 'end-1 index', step)\n",
    "one_to_ten_tensor = torch.arange(1, 11)\n",
    "print(one_to_ten_tensor)\n",
    "ten_to_hundred_tensor = torch.arange(10, 101, 10) # == torch.arange(start=10, end=101, step=10)\n",
    "print(ten_to_hundred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68285845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors like e.g. zeros_like(input=X_tensor)\n",
    "ten_zeros_tensor = torch.zeros_like(one_to_ten_tensor)\n",
    "print(ten_zeros_tensor)\n",
    "print(one_to_ten_tensor.shape)\n",
    "print(ten_zeros_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd09b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], device='cuda:0', dtype=torch.int16)\n",
      "torch.int16\n",
      "\n",
      "tensor([2, 4, 6, 8], device='cuda:0', dtype=torch.int16)\n",
      "torch.int16\n",
      "cuda:0\n",
      "\n",
      "tensor([2, 2, 4, 4], dtype=torch.int32)\n",
      "torch.int32\n",
      "cpu\n",
      "\n",
      "Attempt y2*y3. Should throw an error due to different devices.\n",
      "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! :O\n"
     ]
    }
   ],
   "source": [
    "# Tensor Datatypes\n",
    "y1 = torch.ones(5,3, dtype=torch.int16, device=\"cuda\")\n",
    "y2 = torch.tensor([2.0, 4.0, 6.0, 8.0], \n",
    "               dtype=torch.int16, # What datatype the tensor is\n",
    "               device=\"cuda\", # What device is the tensor on i.e. select device \"cpu\", \"gpu\", \"cuda\", and so on...\n",
    "               requires_grad=False) # Whether or not to track gradients with this tensor's operations\n",
    "y3 = torch.tensor([2.0, 2.0, 4.0, 4.0], \n",
    "               dtype=torch.int32,\n",
    "               device=\"cpu\",\n",
    "               requires_grad=False)\n",
    "print(y1)\n",
    "print(y1.dtype)\n",
    "print()\n",
    "print(y2)\n",
    "print(y2.dtype)\n",
    "print(y2.device)\n",
    "print()\n",
    "print(y3)\n",
    "print(y3.dtype)\n",
    "print(y3.device)\n",
    "print()\n",
    "print(\"Attempt y2*y3. Should throw an error due to different devices.\")\n",
    "try: print(y2*y3)\n",
    "except Exception as e: print(e, \":O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1127e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6., 8.], device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Changing a datatype\n",
    "float_double_y2 = y2.type(torch.double) # == y2.type(torch.float64)\n",
    "print(float_double_y2)\n",
    "print(float_double_y2.shape, float_double_y2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116a048",
   "metadata": {},
   "source": [
    "### Notes - Tensor Data Types\n",
    "There are many torch datatypes. Torch tensors are strictly-typed like Numpy arrays.\n",
    "Refer to https://pytorch.org/docs/stable/tensors.html#torch-tensor for datatypes.\n",
    "Recall the level of precision or max-min trade-off for the bit-size a datatype requires\n",
    "Running a deep learning neural network on a smaller bit-size datatypes typically will be faster, so this embodies the precision vs. speed trade-off.\n",
    "\n",
    "\n",
    "### Common sources of errors (check tensor attributes)\n",
    "1. Tensors are not the correct/compatible datatype - check tensor.dtype\n",
    "2. Tensors are not the correct shape - check tensor.shape (== tensor.size(), a method)\n",
    "e.g. m x n * n x p shape required for matrix mult \n",
    "3. Tensors are not on the right device - check tensor.device\n",
    "e.g. one tensor is on the \"cpu\" and another is on the \"gpu\", and you attempt to perform a matrix operation between these two tensors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5c7ed",
   "metadata": {},
   "source": [
    "## Manipulating Tensors (Tensor operations)\n",
    "Tensor operations include:\n",
    "* Addition\n",
    "* Subtraction\n",
    "* Multiplication (element-wise)\n",
    "* Division\n",
    "* Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f232fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]], device='cuda:0')\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Basic matrix operations\n",
    "z = y1 - 0.5\n",
    "print(z)\n",
    "print((z - 0.5) * 2) # scale to -1 and 1 from {0,1} range \n",
    "#i.e. 0.5 on the range {0, 1} changes to 0 in range {-1,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd26459",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1157)\n",
    "t1 = torch.rand(3,3)\n",
    "t2 = torch.rand(3,3)\n",
    "t3 = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef079826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(2*z) # 2y : matrix*scalar\n",
    "print(2*z + z - z - z) # 2z - z : matrix addition/subtraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a6eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " z: tensor([[0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000]], device='cuda:0')\n",
      "torch.Size([5, 3])\n",
      " t4: tensor([[7.3699e-01, 1.9177e-01, 1.7897e-01],\n",
      "        [1.8632e-01, 5.1089e-01, 9.2811e-01],\n",
      "        [3.8807e-01, 3.9561e-01, 6.9177e-01],\n",
      "        [5.3043e-04, 2.9116e-01, 6.7291e-01],\n",
      "        [2.4199e-01, 8.6214e-01, 8.2421e-01]], device='cuda:0')\n",
      "torch.Size([5, 3])\n",
      " t5: tensor([[0.4517, 0.2861, 0.8093, 0.7285, 0.9133],\n",
      "        [0.3703, 0.2577, 0.7517, 0.5376, 0.6788],\n",
      "        [0.6276, 0.5184, 0.7784, 0.8563, 0.1986]], device='cuda:0')\n",
      "torch.Size([3, 5])\n",
      " element-wise mult, z * t4: tensor([[3.6849e-01, 9.5883e-02, 8.9483e-02],\n",
      "        [9.3158e-02, 2.5545e-01, 4.6406e-01],\n",
      "        [1.9403e-01, 1.9781e-01, 3.4589e-01],\n",
      "        [2.6522e-04, 1.4558e-01, 3.3645e-01],\n",
      "        [1.2100e-01, 4.3107e-01, 4.1210e-01]], device='cuda:0')\n",
      "element-wise mult simply halves the entries of t4 because z is 0.5 in all entries\n",
      " mat-mult (dot product) z * t5: tensor([[0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
      "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
      "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
      "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
      "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954]], device='cuda:0')\n",
      "matrix multiplication (dot product) performs, in a 2D case, a row by col element-wise mult then summation ... \n",
      "... for each entry in the result. E.g. the 1,1 entry is the element-wise multiplication and summation of...\n",
      "... the first row of the first tensor and the first column of the second tensor.\n"
     ]
    }
   ],
   "source": [
    "print(f\" z: {z}\")\n",
    "print(z.shape)\n",
    "t4 = torch.rand(5,3, device=\"cuda\")\n",
    "print(f\" t4: {t4}\")\n",
    "print(t4.shape)\n",
    "t5 = torch.rand(3,5, device=\"cuda\")\n",
    "print(f\" t5: {t5}\")\n",
    "print(t5.shape)\n",
    "print(f\" element-wise mult, z * t4: {z * t4}\")\n",
    "print(\"element-wise mult simply halves the entries of t4 because z is 0.5 in all entries\")\n",
    "print(f\" mat-mult (dot product) z * t5: {torch.matmul(z, t5)}\")\n",
    "print(\"matrix multiplication (dot product) performs, in a 2D case, a row by col element-wise mult then summation ... \")\n",
    "print(\"... for each entry in the result. E.g. the 1,1 entry is the element-wise multiplication and summation of...\")\n",
    "print(\"... the first row of the first tensor and the first column of the second tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c0ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = z\n",
    "tensor_b = t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc370638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 0ns\n",
    "#torch.matmul(tensor_a, tensor_b) == torch.mm(tensor_a, tensor_b) shorthand version\n",
    "torch.mm(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65fff97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954],\n",
       "        [0.7248, 0.5311, 1.1697, 1.0612, 0.8954]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    " # 3.5~4.0ms using cuda\n",
    "# final shape is m x n * n * p => m * p\n",
    "res_m_rows = tensor_a.shape[0]\n",
    "res_p_cols = tensor_b.shape[1]\n",
    "\n",
    "output_tensor = torch.empty((res_m_rows, res_p_cols), device=\"cuda\") \n",
    "# create an empty tensor of m * p dimensions for manual 2D matrix multiplication\n",
    "# print(output_tensor.shape) torch.Size([5, 5])\n",
    "\n",
    "for i, row_i in enumerate(tensor_a):\n",
    "    temp_list_ = [] \n",
    "    # temporary list (a row in the final tensor) to store sums of element-wise row_i * col_j for each i,j combination\n",
    "    for j, col_j in enumerate(torch.t(tensor_b)): \n",
    "        # uses transpose (torch.t) to flip the orientation of tensor_b i.e. rows -> cols, cols -> rows ...\n",
    "        # ... 'for' loops over the first indexing major i.e. rows in Python by default (C: row-major). \n",
    "        # There are packages and methods that use Fortran-major (F: col major) and loop over cols by default.    \n",
    "        val_ = torch.sum(row_i*col_j).item()\n",
    "        # uses torch.sum to sum the individual element-wise multiplications of the elements from row_i * col_j\n",
    "        temp_list_.append(val_) # append each sum to the temporary list\n",
    "    if (len(temp_list_)==res_p_cols):\n",
    "        output_tensor[i,:] = torch.tensor(temp_list_, device=\"cuda\") \n",
    "        # convert temp list (a row) to tensor and insert into output tensor using row index (row i)\n",
    "    else: break # stop if temporary list is of wrong dimension\n",
    "        \n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783a99d",
   "metadata": {},
   "source": [
    "### Other Tensor functions (and other tensor aggregrate operations)\n",
    "(tensor aggregation a tensor -> a smaller amount of #'s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33a0db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random matrix, r:\n",
      "tensor([[-0.5186,  0.4238,  0.4287],\n",
      "        [-0.6329, -0.8228,  0.1959],\n",
      "        [ 0.0624,  0.5950,  0.2551]])\n",
      "\n",
      "Absolute value of r:\n",
      "tensor([[0.5186, 0.4238, 0.4287],\n",
      "        [0.6329, 0.8228, 0.1959],\n",
      "        [0.0624, 0.5950, 0.2551]])\n",
      "\n",
      "Inverse sine of r:\n",
      "tensor([[-0.5453,  0.4376,  0.4430],\n",
      "        [-0.6854, -0.9663,  0.1972],\n",
      "        [ 0.0625,  0.6373,  0.2579]])\n",
      "\n",
      "Determinant of r:\n",
      "tensor(0.1035)\n",
      "\n",
      "Singular value decomposition of r:\n",
      "torch.return_types.svd(\n",
      "U=tensor([[ 0.1407,  0.8751, -0.4630],\n",
      "        [-0.8638,  0.3371,  0.3746],\n",
      "        [ 0.4839,  0.3472,  0.8033]]),\n",
      "S=tensor([1.1722, 0.8874, 0.0995]),\n",
      "V=tensor([[ 0.4299, -0.7275,  0.5348],\n",
      "        [ 0.9028,  0.3382, -0.2657],\n",
      "        [ 0.0124,  0.5970,  0.8022]]))\n",
      "\n",
      "Average and standard deviation of r:\n",
      "(tensor(0.5210), tensor(-0.0015))\n",
      "\n",
      "Minimum value of r:\n",
      "tensor(-0.8228) == tensor(-0.8228)\n",
      "\n",
      "Maximum value of r:\n",
      "tensor(0.5950) == tensor(0.5950)\n",
      "\n",
      "Sum of r:\n",
      "tensor(-0.0135) == tensor(-0.0135)\n"
     ]
    }
   ],
   "source": [
    "r = (torch.rand(3, 3) - 0.5) * 2 # scale values from {0,1} to between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and linear algebra operations like determinant and singular value decomposition\n",
    "print('\\nDeterminant of r:')\n",
    "print(torch.det(r))\n",
    "print('\\nSingular value decomposition of r:')\n",
    "print(torch.svd(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "\n",
    "print('\\nMinimum value of r:')\n",
    "print(torch.min(r), '==', r.min())\n",
    "\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r), '==', r.max())\n",
    "\n",
    "print('\\nSum of r:')\n",
    "print(torch.sum(r), '==', r.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "027b2ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum value of r, each row:\n",
      "torch.return_types.min(\n",
      "values=tensor([-0.5186, -0.8228,  0.0624]),\n",
      "indices=tensor([0, 1, 0]))\n",
      "\n",
      "Minimum value of r, each column:\n",
      "torch.return_types.min(\n",
      "values=tensor([-0.6329, -0.8228,  0.1959]),\n",
      "indices=tensor([1, 1, 1]))\n",
      "\n",
      "Indice of Minimum value of r:\n",
      "index: tensor(4)\n",
      "value: tensor(-0.8228)\n"
     ]
    }
   ],
   "source": [
    "# Locating indices\n",
    "# Example: torch.argmin(X), returns the indices of the minimum value(s) of the flattened tensor or along a dimension\n",
    "\n",
    "print('\\nMinimum value of r, each row:')\n",
    "print(torch.min(r, dim=1)) # min from each row\n",
    "\n",
    "print('\\nMinimum value of r, each column:')\n",
    "print(torch.min(r, dim=0)) # min from each column\n",
    "\n",
    "print('\\nIndice of Minimum value of r:')\n",
    "min_index = torch.argmin(r) # will return the index along a flattened tensor\n",
    "print('index:', min_index)\n",
    "print('value:', torch.flatten(r)[min_index]) # need to flatten the tensor and pass the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ad984cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum value of r, each row:\n",
      "torch.return_types.max(\n",
      "values=tensor([0.4287, 0.1959, 0.5950]),\n",
      "indices=tensor([2, 2, 1]))\n",
      "\n",
      "Maximum value of r, each column:\n",
      "torch.return_types.max(\n",
      "values=tensor([0.0624, 0.5950, 0.4287]),\n",
      "indices=tensor([2, 2, 0]))\n",
      "\n",
      "Indice of Maximum value of r:\n",
      "index: tensor(7)\n",
      "value: tensor(0.5950)\n"
     ]
    }
   ],
   "source": [
    "# Example: torch.argmax(X), returns the indices of the maximum value(s) of the flattened tensor or along a dimension\n",
    "\n",
    "print('\\nMaximum value of r, each row:')\n",
    "print(torch.max(r, dim=1)) # min from each row\n",
    "\n",
    "print('\\nMaximum value of r, each column:')\n",
    "print(torch.max(r, dim=0)) # min from each column\n",
    "\n",
    "print('\\nIndice of Maximum value of r:')\n",
    "max_index = torch.argmax(r) # will return the index along a flattened tensor\n",
    "print('index:', max_index)\n",
    "print('value:', torch.flatten(r)[max_index]) # need to flatten the tensor and pass the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbb3ec",
   "metadata": {},
   "source": [
    "## Reshaping, stacking, squeezing and unsqueezing tensor\n",
    "* Reshaping - reshapes an input tensor to a defined shape\n",
    "* View - return a veiew of an input tensor of certain shape, but keep the same memory as the original tensor (always share the same memory as the original tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
