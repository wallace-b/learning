{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.7.0%2Bcpu-cp37-cp37m-win_amd64.whl (184.2 MB)\n",
      "Requirement already satisfied: future in c:\\python\\anaconda3\\envs\\isye6501\\lib\\site-packages (from torch==1.7.0+cpu) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\python\\anaconda3\\envs\\isye6501\\lib\\site-packages (from torch==1.7.0+cpu) (1.19.2)\n",
      "Collecting torchaudio==0.7.0\n",
      "  Downloading https://download.pytorch.org/whl/torchaudio-0.7.0-cp37-none-win_amd64.whl (103 kB)\n",
      "Collecting torchvision==0.8.1+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.8.1%2Bcpu-cp37-cp37m-win_amd64.whl (808 kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\python\\anaconda3\\envs\\isye6501\\lib\\site-packages (from torchvision==0.8.1+cpu) (8.1.0)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: typing-extensions, dataclasses, torch, torchvision, torchaudio\n",
      "Successfully installed dataclasses-0.6 torch-1.7.0+cpu torchaudio-0.7.0 torchvision-0.8.1+cpu typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "### PyTorch Basics: Tensors & Gradient 01 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install path\n",
    "## !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.float32\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Number\n",
    "t1 = torch.tensor(4.) # by using 4., it creates a floating point number in a 1, size tensor\n",
    "print(t1)\n",
    "print(t1.dtype) # 4. is shorthand for 4.0, so the data_type is float 32-bit, rather than integer\n",
    "print(t1.shape) # shape shows us the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4., 5.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 1D Tensor\n",
    "t2 = torch.tensor([2., 3, 4, 5])\n",
    "print(t2)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 2D / Matrix Tensor\n",
    "t3 = torch.tensor([[1,2], [3,4], [5,6], [7,8]])\n",
    "print(t3)\n",
    "print(t3.shape)\n",
    "\n",
    "# ! Tensors should have a regular shape / consistent shape - for example \n",
    "# t3 = torch.tensor([[1,2], [3,4], [5,6], [7,8,9]]) will pass an error\n",
    "# conversely, a list of lists of lists would happily accept inconsistent internal vector lengths. This is one of...\n",
    "# the key benefits / differences of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11, 12, 13],\n",
      "         [14, 15, 16]],\n",
      "\n",
      "        [[17, 18, 19],\n",
      "         [20, 21, 22]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3D Tensor\n",
    "t4 = torch.tensor([\n",
    "    [[11, 12, 13],\n",
    "     [14, 15, 16]],\n",
    "    [[17, 18, 19], \n",
    "     [20, 21, 22]]])\n",
    "print(t4)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tensor Operations and Gradients ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors\n",
    "x = torch.tensor(3.)\n",
    "y = torch.tensor(4., requires_grad=True)\n",
    "z = torch.tensor(5., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x*y+z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What makes PyTorch unique is that we can automatically compute the derivative of \n",
    "# a w.r.t. the tensors that have requires_grad set to True i.e. y and z. \n",
    "# This feature of PyTorch is called autograd (automatic gradients).\n",
    "\n",
    "# To compute the derivatives, we can invoke the .backward method on our result a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute derivative\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da/dx: None\n",
      "da/dy: tensor(3.)\n",
      "da/dz: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('da/dx:', x.grad)\n",
    "print('da/dy:', y.grad)\n",
    "print('da/dz:', z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As expected, da/dy has the same value as x, i.e., 3, and da/dz has the value 1. \n",
    "# Note that x.grad is None because x doesn't have requires_grad set to True.\n",
    "\n",
    "# The \"grad\" in w.grad is short for gradient, which is another term for derivative. The term gradient is primarily used while dealing with vectors and matrices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
